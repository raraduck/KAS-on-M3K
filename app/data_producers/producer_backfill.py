#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import glob
import csv
import json
import time
import argparse
from datetime import datetime
from kafka import KafkaProducer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
import logging
import psycopg2
from psycopg2.extras import execute_batch
from dotenv import load_dotenv   # âœ… .env íŒŒì¼ ì§€ì›

# .env íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ê¸°ë³¸ ê²½ë¡œ: í˜„ì¬ ì‹¤í–‰ ë””ë ‰í† ë¦¬)
load_dotenv()

# -------------------- ë¡œê±° ì „ì—­ ì„ ì–¸ -------------------- #
logger = None

def setup_logger():
    """ë¡œê±° ì„¤ì •: ì½˜ì†” + íŒŒì¼ ì¶œë ¥"""
    # ë¡œê·¸ ë””ë ‰í„°ë¦¬ ìƒì„±
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)

    # íŒŒì¼ëª…: ì‹¤í–‰ ì‹œê° ê¸°ë°˜
    log_filename = os.path.join(log_dir, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    # ë¡œê±° ìƒì„±
    logger_obj = logging.getLogger()
    logger_obj.setLevel(logging.INFO)

    # í¬ë§· ì§€ì •
    formatter = logging.Formatter(
        fmt="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ (ì½˜ì†”ìš©)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.flush = sys.stdout.flush  # âœ… ì¦‰ì‹œ ì¶œë ¥ìš©

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ë¡œê·¸íŒŒì¼ìš©)
    file_handler = logging.FileHandler(log_filename, mode='w', encoding='utf-8')
    file_handler.setFormatter(formatter)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ì¬ë“±ë¡ (ì¤‘ë³µ ë°©ì§€)
    if logger_obj.hasHandlers():
        logger_obj.handlers.clear()
    logger_obj.addHandler(console_handler)
    logger_obj.addHandler(file_handler)

    logging.info(f"ğŸ§¾ Logging started: {log_filename}")

    return logger_obj


# logger íŒŒì¼ë¡œ ì €ì¥
# try catch ìì„¸í•˜ê²Œ ì ìš©

# -------------------- í† í”½ ìƒì„± -------------------- #
def create_topic(dest_servers, topic_name, num_partitions=3, replication_factor=3):
    admin_client = KafkaAdminClient(
        bootstrap_servers=dest_servers,
        client_id='topic_creator'
    )

    topic = NewTopic(
        name=topic_name,
        num_partitions=num_partitions,
        replication_factor=replication_factor
    )

    try:
        admin_client.create_topics(new_topics=[topic], validate_only=False)
        logger.info(f"âœ… í† í”½ ìƒì„± ì™„ë£Œ: {topic_name} (partitions={num_partitions}, replicas={replication_factor})")
    except TopicAlreadyExistsError:
        logger.warn(f"âš ï¸ í† í”½ '{topic_name}'ì€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    finally:
        admin_client.close()


# -------------------- JSON ì§ë ¬í™” -------------------- #
def json_serializer(data):
    """ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì§ë ¬í™”"""
    return json.dumps(data).encode('utf-8')


# -------------------- CSV íŒŒì¼ ì œë„ˆë ˆì´í„° -------------------- #
def iter_all_csv_rows(base_dir):
    """
    data/machine-*-*/*_train.csv íŒŒì¼ì„ ì „ë¶€ ìˆœíšŒí•˜ë©° ê° row yield
    """
    data_pattern = os.path.join(base_dir, "data", "machine-*", "*_train.csv")
    csv_files = sorted(glob.glob(data_pattern))

    if not csv_files:
        logger.warn(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {data_pattern}")
        return

    for csv_path in csv_files:
        machine = os.path.basename(os.path.dirname(csv_path))
        logger.info(f"ğŸ“‚ ì½ëŠ” ì¤‘: {csv_path}")

        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                numeric_row = {k: try_parse_number(v) for k, v in row.items()}
                numeric_row["send_timestamp"] = datetime.now().isoformat()
                numeric_row["machine"] = f"{machine}-train"
                yield numeric_row, machine  # machine ì´ë¦„ë„ ë°˜í™˜


def try_parse_number(value):
    """ë¬¸ìì—´ì„ float/intë¡œ ë³€í™˜, ì‹¤íŒ¨ ì‹œ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    try:
        if "." in value or "e" in value or "E" in value:
            return float(value)
        else:
            return int(value)
    except Exception:
        return value


# -------------------- Kafka ì „ì†¡ ì½œë°± -------------------- #
def on_send_success(record_metadata):
    logger.info(f"âœ… ì„±ê³µ: topic={record_metadata.topic}, partition={record_metadata.partition}, offset={record_metadata.offset}")

def on_send_error(excp):
    logger.error(f"âŒ ì‹¤íŒ¨: {excp}")


# -------------------- ë©”ì¸ ì¹´í”„ì¹´ ë£¨í”„ -------------------- #
def main_kafka(args):
    global logger

    dest_servers = args.dest_servers.split(",")
    topic_name = args.topic

    # âœ… í† í”½ ìë™ ìƒì„±
    create_topic(
        dest_servers, 
        topic_name, 
        num_partitions=14, 
        replication_factor=1
    )

    # âœ… Kafka Producer ì„¤ì • (ì§€ì—° ìµœì†Œí™”, ë³‘ë ¬ ìµœì í™”)
    producer = KafkaProducer(
        bootstrap_servers=dest_servers,
        value_serializer=json_serializer,
        key_serializer=str.encode,
        acks='all',                   # ì™„ì „ ë³´ì¥
        retries=3,
        linger_ms=1000,                  # ì¦‰ì‹œ ì „ì†¡
        batch_size=16384,
        request_timeout_ms=20000
        # client_id="backfill-producer",
        # bootstrap_servers=dest_servers,
        # key_serializer=str.encode,
        # value_serializer=json_serializer,
        # acks='1',  # ì†ë„ â†‘ (acks=all ë³´ë‹¤ ë¹ ë¦„)
        # # linger_ms=5,  # ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„ (5ms)
        # # batch_size=32768,  # 32KB
        # # compression_type='gzip',  # CPU ë¶€í•˜ ì ê³  ë¹ ë¦„
        # # max_in_flight_requests_per_connection=5
    )

    base_dir = os.path.dirname(os.path.abspath(__file__))
    total_sent = 0
    start_time = time.time()
    
    logger.info("ğŸš€ Kafka Producer ì‹œì‘ (ëª¨ë“  machine-* CSV ë³‘ë ¬ ì „ì†¡). Ctrl+Cë¡œ ì¢…ë£Œ.")

    try:
        for record, machine in iter_all_csv_rows(base_dir):
            future = producer.send(
                topic_name,
                key=f"{machine}-train",  # íŒŒí‹°ì…˜ ê· ë“± ë¶„ì‚°ì„ ìœ„í•œ key
                value=record
            )
            future.add_callback(on_send_success).add_errback(on_send_error)
            total_sent += 1
            if total_sent % 500 == 0:
                producer.flush()

        producer.flush()
        elapsed = time.time() - start_time
        logger.info(f"\nâœ… ì „ì†¡ ì™„ë£Œ: {total_sent} rows / {elapsed:.2f}ì´ˆ / í‰ê·  {total_sent/elapsed:.1f} msg/sec")

    except KeyboardInterrupt:
        logger.error("ğŸ›‘ í”„ë¡œë“€ì„œ ì¢…ë£Œ (ìˆ˜ë™ ì¤‘ë‹¨)")
    finally:
        producer.flush()
        producer.close()

# -------------------- ë©”ì¸ postgresql ë£¨í”„ -------------------- #
def main_postgres(args):
    global logger

    """CSV ë°ì´í„°ë¥¼ PostgreSQLì— ì§ì ‘ ì €ì¥"""
    base_dir = os.path.dirname(os.path.abspath(__file__))

    # âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env)
    load_dotenv()

    PG_HOST = args.pg_host
    PG_PORT = args.pg_port
    PG_DB = args.pg_db
    PG_USER = args.pg_user
    PG_PASS = args.pg_pass
    PG_TABLE = args.pg_table

    conn = psycopg2.connect(
        host=PG_HOST,
        port=PG_PORT,
        dbname=PG_DB,
        user=PG_USER,
        password=PG_PASS
    )
    cur = conn.cursor()

    # âœ… í…Œì´ë¸” ìƒì„± (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {PG_TABLE} (
        id SERIAL PRIMARY KEY,
        send_timestamp TIMESTAMPTZ,
        machine TEXT,
        timestamp TEXT,
        label INT,
        {','.join([f'col_{i} FLOAT' for i in range(38)])}
    );
    """
    cur.execute(create_sql)
    conn.commit()

    total_inserted = 0
    start_time = time.time()
    logger.info(f"ğŸš€ PostgreSQL ì €ì¥ ì‹œì‘: {PG_HOST}:{PG_PORT}/{PG_DB} â†’ {PG_TABLE}")

    try:
        batch = []
        for record, machine in iter_all_csv_rows(base_dir):
            cols = ["send_timestamp", "machine", "timestamp", "label"] + [f"col_{i}" for i in range(38)]
            values = [record.get(c, None) for c in cols]
            batch.append(values)

            if len(batch) >= 500:
                placeholders = ",".join(["%s"] * len(cols))
                insert_sql = f"INSERT INTO {PG_TABLE} ({','.join(cols)}) VALUES ({placeholders})"
                execute_batch(cur, insert_sql, batch)
                conn.commit()
                total_inserted += len(batch)
                logger.info(f"ğŸ’¾ {total_inserted} rows inserted...")
                batch.clear()

        if batch:
            placeholders = ",".join(["%s"] * len(cols))
            insert_sql = f"INSERT INTO {PG_TABLE} ({','.join(cols)}) VALUES ({placeholders})"
            execute_batch(cur, insert_sql, batch)
            conn.commit()
            total_inserted += len(batch)
            batch.clear()

        elapsed = time.time() - start_time
        logger.info(f"\nâœ… ì €ì¥ ì™„ë£Œ: {total_inserted} rows / {elapsed:.2f}ì´ˆ / í‰ê·  {total_inserted/elapsed:.1f} row/sec")

    except KeyboardInterrupt:
        logger.error("ğŸ›‘ ìˆ˜ë™ ì¢…ë£Œ")
    finally:
        cur.close()
        conn.close()

# -------------------- ì‹¤í–‰ -------------------- #
if __name__ == "__main__":
    logger = setup_logger()

    parser = argparse.ArgumentParser(description='Kafka ë˜ëŠ” PostgreSQL - ëª¨ë“  machine CSV ì „ì†¡')
    parser.add_argument('--dest', choices=['postgresql', 'kafka'], default='kafka', type=str, help='ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ê³³ (postgresql, kafka)')
    parser.add_argument('--topic', default='backfill-train-topic', type=str, help='ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ í† í”½')
    parser.add_argument('--dest-servers', default='kafka.kafka.svc.cluster.local:9092',
                        type=str, help='Kafka ë˜ëŠ” Postgres ì„œë²„')
    
    # âœ… PostgreSQL ì¸ì ì¶”ê°€
    parser.add_argument("--pg-host", default=os.getenv("PG_HOST", "localhost"), help='PostgreSQL í˜¸ìŠ¤íŠ¸ëª…')
    parser.add_argument("--pg-port", type=int, default=int(os.getenv("PG_PORT", 5432)), help='PostgreSQL í¬íŠ¸')
    parser.add_argument("--pg-db", default=os.getenv("PG_DB", "postgres"), help='PostgreSQL DBëª…')
    parser.add_argument("--pg-user", default=os.getenv("PG_USER", "postgres"), help='PostgreSQL ì‚¬ìš©ìëª…')
    parser.add_argument("--pg-pass", default=os.getenv("PG_PASS", "postgres"), help='PostgreSQL ë¹„ë°€ë²ˆí˜¸')
    parser.add_argument("--pg-table", default=os.getenv("PG_TABLE", "smd_table_realtime"), help='PostgreSQL í…Œì´ë¸”ëª…')

    args = parser.parse_args()

    if args.dest == 'kafka':
        logger.info("Kafka ì „ì†¡ ì‹œì‘")
        main_kafka(args)
    elif args.dest == 'postgresql':
        logger.info("PostgreSQL ì €ì¥ ì‹œì‘")
        main_postgres(args)
    else:
        logger.error(f"âŒ ì˜ëª»ëœ dest ì¸ì: {args.dest}")
        sys.exit(1)
