#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import glob
import csv
import json
import time
import argparse
from datetime import datetime
from kafka import KafkaProducer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError


# logger íŒŒì¼ë¡œ ì €ì¥
# try catch ìì„¸í•˜ê²Œ ì ìš©

# -------------------- í† í”½ ìƒì„± -------------------- #
def create_topic(bootstrap_servers, topic_name, num_partitions=14, replication_factor=3):
    admin_client = KafkaAdminClient(
        bootstrap_servers=bootstrap_servers,
        client_id='topic_creator'
    )

    topic = NewTopic(
        name=topic_name,
        num_partitions=num_partitions,
        replication_factor=replication_factor
    )

    try:
        admin_client.create_topics(new_topics=[topic], validate_only=False)
        print(f"âœ… í† í”½ ìƒì„± ì™„ë£Œ: {topic_name} (partitions={num_partitions}, replicas={replication_factor})")
    except TopicAlreadyExistsError:
        print(f"âš ï¸ í† í”½ '{topic_name}'ì€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    finally:
        admin_client.close()


# -------------------- JSON ì§ë ¬í™” -------------------- #
def json_serializer(data):
    """ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì§ë ¬í™”"""
    return json.dumps(data).encode('utf-8')


# -------------------- CSV íŒŒì¼ ì œë„ˆë ˆì´í„° -------------------- #
def iter_all_csv_rows(base_dir):
    """
    data/machine-*-*/*_train.csv íŒŒì¼ì„ ì „ë¶€ ìˆœíšŒí•˜ë©° ê° row yield
    """
    data_pattern = os.path.join(base_dir, "data", "machine-*", "*_train.csv")
    csv_files = sorted(glob.glob(data_pattern))

    if not csv_files:
        print(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {data_pattern}")
        return

    for csv_path in csv_files:
        machine = os.path.basename(os.path.dirname(csv_path))
        print(f"ğŸ“‚ ì½ëŠ” ì¤‘: {csv_path}")

        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                numeric_row = {k: try_parse_number(v) for k, v in row.items()}
                numeric_row["send_timestamp"] = datetime.now().isoformat()
                numeric_row["machine"] = f"{machine}-train"
                yield numeric_row, machine  # machine ì´ë¦„ë„ ë°˜í™˜


def try_parse_number(value):
    """ë¬¸ìì—´ì„ float/intë¡œ ë³€í™˜, ì‹¤íŒ¨ ì‹œ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    try:
        if "." in value or "e" in value or "E" in value:
            return float(value)
        else:
            return int(value)
    except Exception:
        return value


# -------------------- Kafka ì „ì†¡ ì½œë°± -------------------- #
def on_send_success(record_metadata):
    print(f"âœ… ì„±ê³µ: topic={record_metadata.topic}, partition={record_metadata.partition}, offset={record_metadata.offset}")

def on_send_error(excp):
    print(f"âŒ ì‹¤íŒ¨: {excp}")


# -------------------- ë©”ì¸ ë£¨í”„ -------------------- #
def main():
    parser = argparse.ArgumentParser(description='Kafka í”„ë¡œë“€ì„œ - ëª¨ë“  machine CSV ì „ì†¡')
    parser.add_argument('--topic', default='backfill-train-topic', type=str, help='ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ í† í”½')
    parser.add_argument('--bootstrap-servers', default='kafka.kafka.svc.cluster.local:9092',
                        type=str, help='Kafka ë¶€íŠ¸ìŠ¤íŠ¸ë© ì„œë²„')
    args = parser.parse_args()

    bootstrap_servers = args.bootstrap_servers.split(",")
    topic_name = args.topic

    # âœ… í† í”½ ìë™ ìƒì„±
    create_topic(bootstrap_servers, topic_name, num_partitions=14, replication_factor=3)

    # âœ… Kafka Producer ì„¤ì • (ì§€ì—° ìµœì†Œí™”, ë³‘ë ¬ ìµœì í™”)
    producer = KafkaProducer(
        client_id="backfill-producer",
        bootstrap_servers=bootstrap_servers,
        key_serializer=str.encode,
        value_serializer=json_serializer,
        acks='1',  # ì†ë„ â†‘ (acks=all ë³´ë‹¤ ë¹ ë¦„)
        # linger_ms=5,  # ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„ (5ms)
        # batch_size=32768,  # 32KB
        # compression_type='gzip',  # CPU ë¶€í•˜ ì ê³  ë¹ ë¦„
        # max_in_flight_requests_per_connection=5
    )

    base_dir = os.path.dirname(os.path.abspath(__file__))
    total_sent = 0
    start_time = time.time()
    
    print("ğŸš€ Kafka Producer ì‹œì‘ (ëª¨ë“  machine-* CSV ë³‘ë ¬ ì „ì†¡). Ctrl+Cë¡œ ì¢…ë£Œ.")

    try:
        for record, machine in iter_all_csv_rows(base_dir):
            future = producer.send(
                topic_name,
                key=f"{machine}-train",  # íŒŒí‹°ì…˜ ê· ë“± ë¶„ì‚°ì„ ìœ„í•œ key
                value=record
            )
            future.add_callback(on_send_success).add_errback(on_send_error)
            total_sent += 1

        producer.flush()
        elapsed = time.time() - start_time
        print(f"\nâœ… ì „ì†¡ ì™„ë£Œ: {total_sent} rows / {elapsed:.2f}ì´ˆ / í‰ê·  {total_sent/elapsed:.1f} msg/sec")

    except KeyboardInterrupt:
        print("ğŸ›‘ í”„ë¡œë“€ì„œ ì¢…ë£Œ (ìˆ˜ë™ ì¤‘ë‹¨)")
    finally:
        producer.flush()
        producer.close()


# -------------------- ì‹¤í–‰ -------------------- #
if __name__ == "__main__":

    main()
