#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import glob
import csv
import json
import time
import argparse
from datetime import datetime
from kafka import KafkaProducer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
import logging

# -------------------- ë¡œê±° ì „ì—­ ì„ ì–¸ -------------------- #
logger = None

def setup_logger():
    """ë¡œê±° ì„¤ì •: ì½˜ì†” + íŒŒì¼ ì¶œë ¥"""
    # ë¡œê·¸ ë””ë ‰í„°ë¦¬ ìƒì„±
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)

    # íŒŒì¼ëª…: ì‹¤í–‰ ì‹œê° ê¸°ë°˜
    log_filename = os.path.join(log_dir, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    # ë¡œê±° ìƒì„±
    logger_obj = logging.getLogger()
    logger_obj.setLevel(logging.INFO)

    # í¬ë§· ì§€ì •
    formatter = logging.Formatter(
        fmt="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ (ì½˜ì†”ìš©)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.flush = sys.stdout.flush  # âœ… ì¦‰ì‹œ ì¶œë ¥ìš©

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ë¡œê·¸íŒŒì¼ìš©)
    file_handler = logging.FileHandler(log_filename, mode='w', encoding='utf-8')
    file_handler.setFormatter(formatter)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ì¬ë“±ë¡ (ì¤‘ë³µ ë°©ì§€)
    if logger_obj.hasHandlers():
        logger_obj.handlers.clear()
    logger_obj.addHandler(console_handler)
    logger_obj.addHandler(file_handler)

    logging.info(f"ğŸ§¾ Logging started: {log_filename}")

    return logger_obj

# -------------------- í† í”½ ìƒì„± -------------------- #
def create_topic(bootstrap_servers, topic_name, num_partitions=3, replication_factor=1):
    admin_client = KafkaAdminClient(
        bootstrap_servers=bootstrap_servers,
        client_id='topic_creator'
    )

    topic = NewTopic(
        name=topic_name,
        num_partitions=num_partitions,
        replication_factor=replication_factor
    )

    try:
        admin_client.create_topics(new_topics=[topic], validate_only=False)
        logger.info(f"âœ… í† í”½ ìƒì„± ì™„ë£Œ: {topic_name}")
    except TopicAlreadyExistsError:
        logger.warning(f"âš ï¸ í† í”½ '{topic_name}'ì€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    finally:
        admin_client.close()


# -------------------- JSON ì§ë ¬í™” -------------------- #
def json_serializer(data):
    """ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì§ë ¬í™”"""
    return json.dumps(data).encode('utf-8')


# -------------------- CSV íŒŒì¼ ì œë„ˆë ˆì´í„° -------------------- #
def iter_smd_csv_rows(machine):
    """
    data/machine-*-*/ í•˜ìœ„ì˜ *_test.csv íŒŒì¼ì„ ìˆœíšŒí•˜ë©°
    ê° íŒŒì¼ì˜ í•œ ì¤„(row)ì„ yield
    """
    base_dir = os.path.dirname(os.path.abspath(__file__))
    data_pattern = os.path.join(base_dir, "data", machine, "*_test.csv")
    csv_files = sorted(glob.glob(data_pattern))

    if not csv_files:
        logger.warning(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {data_pattern}")
        return

    for csv_path in csv_files:
        logger.info(f"ğŸ“‚ ì½ëŠ” ì¤‘: {os.path.basename(csv_path)}")

        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # ê° í–‰ì„ float ë˜ëŠ” intë¡œ ë³€í™˜
                numeric_row = {k: try_parse_number(v) for k, v in row.items()}
                # CSVì˜ timestamp ëŒ€ì‹  ì „ì†¡ ì‹œê°ì„ ë®ì–´ì“°ê¸° (ì„ íƒ)
                numeric_row["send_timestamp"] = datetime.now().isoformat()
                numeric_row["machine"] = machine
                numeric_row["usage"] = f"test"
                yield numeric_row


def try_parse_number(value):
    """ë¬¸ìì—´ì„ float/intë¡œ ë³€í™˜, ì‹¤íŒ¨ ì‹œ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    try:
        if "." in value or "e" in value or "E" in value:
            return float(value)
        else:
            return int(value)
    except Exception:
        return value


# -------------------- Kafka ì „ì†¡ ì½œë°± -------------------- #
def on_send_success(record_metadata):
    logger.info(f"âœ… ì„±ê³µ: topic={record_metadata.topic}, partition={record_metadata.partition}, offset={record_metadata.offset}")

def on_send_error(excp):
    logger.error(f"âŒ ì‹¤íŒ¨: {excp}")


# -------------------- ë©”ì¸ ë£¨í”„ -------------------- #
def main():
    global logger
    # ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='Kafka í”„ë¡œë“€ì„œ ì˜ˆì œ - ë©”ì‹œì§€ ìƒì„±')
    parser.add_argument('--topic', default='test-topic', type=str, help='ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ í† í”½')
    parser.add_argument('--interval', default=60, type=int, help='ë©”ì‹œì§€ ì „ì†¡ ê°„ê²© (ë‹¨ìœ„ ì´ˆ)')
    parser.add_argument('--machine', default='machine-1-1', type=str, help='ì¸¡ì •í•  ë¨¸ì‹  ì´ë¦„ ex. machine-*-*')
    parser.add_argument('--bootstrap-servers', default='kafka.kafka.svc.cluster.local:9092',
                     type=str, help='Kafka ë¶€íŠ¸ìŠ¤íŠ¸ë© ì„œë²„')
    parser.add_argument('--partitions', default=1, type=int, help='í† í”½ íŒŒí‹°ì…˜ ìˆ˜ (ê¸°ë³¸: 1)')
    parser.add_argument('--replications', default=3, type=int, help='í† í”½ ë³µì œë³¸ ìˆ˜ (ê¸°ë³¸: 3)')
    args = parser.parse_args()
    
    bootstrap_servers = args.bootstrap_servers.split(",") # ['kafka.kafka.svc.cluster.local:9092']
    topic_name = args.topic # "realtime-test-topic"

    create_topic(
        bootstrap_servers=bootstrap_servers,
        topic_name=topic_name,
        num_partitions=args.partitions,
        replication_factor=args.replications
    )

    producer = KafkaProducer(
        client_id="machine-producer",   # âœ… í”„ë¡œë“€ì„œ ì‹ë³„ìš© ì´ë¦„
        bootstrap_servers=bootstrap_servers,
        key_serializer=str.encode,         # âœ… ë¬¸ìì—´ â†’ bytes ìë™ ë³€í™˜
        value_serializer=json_serializer,
        acks='all'
    )

    logger.info("ğŸš€ Kafka Producer ì‹œì‘ (ë¬´í•œ ë°˜ë³µ). Ctrl+Cë¡œ ì¢…ë£Œ.")

    try:
        while True:  # ğŸ” ë¬´í•œ ë£¨í”„
            for message in iter_smd_csv_rows(args.machine):
                # Kafkaë¡œ ì „ì†¡
                future = producer.send(
                    topic_name, 
                    value=message, 
                    key=f"{args.machine}-test" # f"{args.machine}".encode("utf-8")
                )  # ë°˜ë“œì‹œ bytes í˜•ì‹
                future.add_callback(on_send_success).add_errback(on_send_error)

                logger.info(f"ğŸ“¤ ì „ì†¡: {message}")
                time.sleep(args.interval)  # ì „ì†¡ ê°„ê²© ì¡°ì • ê°€ëŠ¥ (ë¶„ë‹¹ 1ê±´)
            
            # í•œ ë°”í€´ ë‹¤ ëŒì•˜ìœ¼ë©´ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹œì‘
            logger.info("ğŸ” CSV ì „ì²´ ì „ì†¡ ì™„ë£Œ. Inverval (ex. 60ì´ˆ) í›„ ì¬ì‹œì‘...\n")
            time.sleep(args.interval)

    except KeyboardInterrupt:
        logger.error("ğŸ›‘ í”„ë¡œë“€ì„œ ì¢…ë£Œ")
    finally:
        producer.flush()
        producer.close()


# -------------------- ì‹¤í–‰ -------------------- #
if __name__ == "__main__":
    logger = setup_logger()

    main()
