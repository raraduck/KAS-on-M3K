#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import sys
from kafka import KafkaConsumer
import json
import psycopg2
from psycopg2.extras import execute_batch
import pandas as pd
from datetime import datetime
import argparse
import os
from dotenv import load_dotenv
import logging

# .env íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ê¸°ë³¸ ê²½ë¡œ: í˜„ì¬ ì‹¤í–‰ ë””ë ‰í† ë¦¬)
load_dotenv()

# -------------------- ë¡œê±° ì „ì—­ ì„ ì–¸ -------------------- #
logger = None

def setup_logger():
    """ë¡œê±° ì„¤ì •: ì½˜ì†” + íŒŒì¼ ì¶œë ¥"""
    # ë¡œê·¸ ë””ë ‰í„°ë¦¬ ìƒì„±
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)

    # íŒŒì¼ëª…: ì‹¤í–‰ ì‹œê° ê¸°ë°˜
    log_filename = os.path.join(log_dir, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    # ë¡œê±° ìƒì„±
    logger_obj = logging.getLogger()
    logger_obj.setLevel(logging.INFO)

    # í¬ë§· ì§€ì •
    formatter = logging.Formatter(
        fmt="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ (ì½˜ì†”ìš©)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.flush = sys.stdout.flush  # âœ… ì¦‰ì‹œ ì¶œë ¥ìš©

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ë¡œê·¸íŒŒì¼ìš©)
    file_handler = logging.FileHandler(log_filename, mode='w', encoding='utf-8')
    file_handler.setFormatter(formatter)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ì¬ë“±ë¡ (ì¤‘ë³µ ë°©ì§€)
    if logger_obj.hasHandlers():
        logger_obj.handlers.clear()
    logger_obj.addHandler(console_handler)
    logger_obj.addHandler(file_handler)

    logging.info(f"ğŸ§¾ Logging started: {log_filename}")

    return logger_obj

# -------------------- PostgreSQL ì—°ê²° ì •ë³´ -------------------- #
# PG_CONFIG = {
#     "host": "airflow-postgresql.airflow.svc.cluster.local",
#     "port": 5432,
#     "dbname": "postgres",
#     "user": "postgres",
#     "password": "postgres"
# }
# TABLE_NAME = "smd_table_realtime"

# -------------------- JSON ì—­ì§ë ¬í™” -------------------- #
def json_deserializer(data):
    """Kafka ë©”ì‹œì§€ë¥¼ JSONìœ¼ë¡œ ì—­ì§ë ¬í™”"""
    try:
        return json.loads(data.decode("utf-8"))
    except Exception as e:
        logger.warnming(f"âš ï¸ JSON ë””ì½”ë”© ì˜¤ë¥˜: {e}")
        return None

# -------------------- DB ì €ì¥ í•¨ìˆ˜ -------------------- #
def save_to_postgres(df, pg_config, table_name):
    """pandas DataFrameì„ PostgreSQLì— overwrite ì €ì¥"""
    if df.empty:
        logger.warnming("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    conn = psycopg2.connect(**pg_config)
    cur = conn.cursor()

    # ë™ì  í…Œì´ë¸” ìƒì„± (ì—†ì„ ì‹œ)
    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        id SERIAL PRIMARY KEY,
        send_timestamp TIMESTAMPTZ,
        machine TEXT,
        timestamp TEXT,
        usage TEXT,
        label INT,
        {','.join([f'col_{i} FLOAT' for i in range(38)])}
    );
    """
    cur.execute(create_sql)

    # # ê¸°ì¡´ í…Œì´ë¸” ë®ì–´ì“°ê¸°(Overwrite)
    # cur.execute(f"TRUNCATE TABLE {TABLE_NAME};")

    # ì»¬ëŸ¼ëª… êµ¬ì„±
    cols = [f"col_{i}" for i in range(38)]
    col_names = ["send_timestamp", "machine", "timestamp", "usage", "label"] + cols
    placeholders = ", ".join(["%s"] * len(col_names))

    # DataFrame â†’ list of tuples
    records = []
    for _, row in df.iterrows():
        record = [
            row.get("send_timestamp"),
            row.get("machine"),
            row.get("timestamp"),
            row.get("usage"),
            row.get("label")
        ]
        record += [row.get(c) for c in cols]
        records.append(tuple(record))

    # assert len(record) == len(col_names)

    # Batch insert (ì„±ëŠ¥ ê°œì„ )
    execute_batch(
        cur,
        f"INSERT INTO {table_name} ({', '.join(col_names)}) VALUES ({placeholders})",
        records
    )

    conn.commit()
    cur.close()
    conn.close()
    logger.info(f"ğŸ’¾ {len(df)}ê±´ì„ PostgreSQL '{table_name}' í…Œì´ë¸”ì— overwrite ì €ì¥ ì™„ë£Œ")

# -------------------- ë©”ì‹œì§€ ì²˜ë¦¬ -------------------- #
def process_message(message):
    """Kafka ë©”ì‹œì§€ë¥¼ Python dictë¡œ ë³€í™˜"""
    try:
        send_ts = message.get("send_timestamp")
        machine = message.get("machine")
        timestamp = message.get("timestamp")
        usage = message.get("usage")
        label = int(message.get("label", 0))
        cols = {k: v for k, v in message.items() if k.startswith("col_")}

        # send_timestampëŠ” ë¬¸ìì—´ í˜•íƒœë¡œ ì˜¬ ê²½ìš° ê·¸ëŒ€ë¡œ DBê°€ ì²˜ë¦¬ ê°€ëŠ¥
        return {"send_timestamp": send_ts, "machine": machine, "timestamp": timestamp, "usage": usage, "label": label, **cols}

    except Exception as e:
        logger.warnming(f"âš ï¸ ë©”ì‹œì§€ íŒŒì‹± ì˜¤ë¥˜: {e}\nì›ë³¸: {message}")
        return None

# -------------------- ë©”ì¸ -------------------- #
def main():
    global logger
    parser = argparse.ArgumentParser(description="Kafka â†’ PostgreSQL Consumer")

    # Kafka ì„¤ì •
    parser.add_argument('--topic', default='test-topic', type=str, help='ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ í† í”½')
    parser.add_argument('--bootstrap-servers', default='kafka.kafka.svc.cluster.local:9092',
                     type=str, help='Kafka ë¶€íŠ¸ìŠ¤íŠ¸ë© ì„œë²„')
    parser.add_argument("--group-id", default="smd-realtime-group", help="Kafka consumer group ID")
    parser.add_argument("--timeout", type=int, default=90000, help='ë©”ì‹œì§€ íƒ€ì„ì•„ì›ƒ (ë‹¨ìœ„ ë°€ë¦¬ì´ˆ), default: 90000')

    # PostgreSQL ì„¤ì •
    parser.add_argument("--pg-host", default=os.getenv("PG_HOST", "localhost"))
    parser.add_argument("--pg-port", type=int, default=int(os.getenv("PG_PORT", 5432)))
    parser.add_argument("--pg-db", default=os.getenv("PG_DB", "postgres"))
    parser.add_argument("--pg-user", default=os.getenv("PG_USER", "postgres"))
    parser.add_argument("--pg-pass", default=os.getenv("PG_PASS", "postgres"))
    parser.add_argument("--pg-table", default=os.getenv("PG_TABLE", "smd_table_realtime"))

    parser.add_argument("--batch-size", type=int, default=100, help="Postgresë¡œ ì €ì¥í•  batch í¬ê¸°")

    args = parser.parse_args()

    pg_config = {
        "host": args.pg_host,
        "port": args.pg_port,
        "dbname": args.pg_db,
        "user": args.pg_user,
        "password": args.pg_pass,
    }

    consumer = KafkaConsumer(
        args.topic,
        bootstrap_servers=args.bootstrap_servers.split(","), # "kafka.kafka.svc.cluster.local:9092",
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        group_id=args.group_id, # "smd-consumer-group",
        value_deserializer=json_deserializer,
        consumer_timeout_ms=args.timeout
    )

    logger.info("ğŸš€ Kafka â†’ PostgreSQL Consumer ì‹œì‘.")
    buffer = []

    try:
        for message in consumer:
            data = process_message(message.value)
            if data:
                buffer.append(data)

            # 100ê±´ ë‹¨ìœ„ë¡œ DB ì €ì¥
            if len(buffer) >= args.batch_size:
                df = pd.DataFrame(buffer)
                save_to_postgres(df, pg_config, args.pg_table)
                buffer.clear()

    except KeyboardInterrupt:
        logger.error("ğŸ›‘ ì»¨ìŠˆë¨¸ ìˆ˜ë™ ì¢…ë£Œ ìš”ì²­")
    finally:
        # ì”ì—¬ ë²„í¼ ì²˜ë¦¬
        if buffer:
            df = pd.DataFrame(buffer)
            save_to_postgres(df, pg_config, args.pg_table)
        consumer.close()
        logger.info("âœ… Kafka Consumer ì¢…ë£Œ ì™„ë£Œ")


if __name__ == "__main__":
    logger = setup_logger()
    main()
