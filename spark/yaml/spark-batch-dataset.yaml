apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-batch-dataset          # ğŸ’¡ job ì´ë¦„ ë³€ê²½
  namespace: default
spec:
  sparkConf:
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.endpoint": "s3.ap-northeast-2.amazonaws.com"

  type: Python
  mode: cluster
  image: dwnusa/spark:v3.5.4-amd64
  imagePullPolicy: IfNotPresent
  mainApplicationFile: local:///opt/spark-data/spark_batch_dataset.py
  sparkVersion: 3.5.4

  restartPolicy:
    type: Never

  arguments:             # âœ… Python script ì¸ì ì „ë‹¬ (PostgreSQL â†’ S3)
    # PostgreSQL ì—°ê²°ì •ë³´
    - "--pg-table"
    - "backfill_train_251112"

    # S3 ì €ì¥ ê´€ë ¨
    - "--s3-bucket"
    - "s3a://kas-on-m3k/smd-dataset"
    - "--format"
    - "parquet"

    # ìµœê·¼ Nì¼ë§Œ ë°±ì—…
    - "--days"
    - "3"

  driver:
    cores: 1
    memory: 2g
    serviceAccount: spark-operator-spark
    volumeMounts:
      - name: data
        mountPath: /opt/spark-data

  executor:
    cores: 1
    instances: 2            # executor ìˆ˜ ì¦ê°€
    memory: 2g
    volumeMounts:
      - name: data
        mountPath: /opt/spark-data

  volumes:
    - name: data
      hostPath:
        path: /opt/spark/jobs
