apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-batch-backfill
  namespace: default # airflow # spark-operator
spec:
  type: Python
  mode: cluster
  # image: docker.io/library/spark:4.0.0
  image: dwnusa/spark:v3.5.4-amd64
  imagePullPolicy: IfNotPresent
  mainApplicationFile: local:///opt/spark-data/spark_batch_backfill.py
  sparkVersion: 3.5.4
  restartPolicy:
    type: Never
  arguments:             # ✅ Python script 인자 전달
    - "--pg-host"
    - "10.246.246.33"
    - "--pg-port"
    - "12345"
    - "--pg-db"
    - "testdb"
    - "--pg-user"
    - "dwnusa"
    - "--pg-table"
    - "smd_raw_data_lake"
    - "--kafka-bootstrap"
    - "kafka.kafka.svc.cluster.local:9092"
    - "--topic"
    - "backfill-kafka"
    - "--days"
    - "1"
  driver:
    cores: 1
    memory: 512m
    serviceAccount: spark-operator-spark
    volumeMounts:
      - name: data
        mountPath: /opt/spark-data
  executor:
    cores: 1
    instances: 4
    memory: 512m
#     securityContext:
#       runAsUser: 0
#       runAsGroup: 0
    volumeMounts:
      - name: data
        mountPath: /opt/spark-data
  volumes:
    - name: data
      hostPath:
        path: /opt/spark/jobs