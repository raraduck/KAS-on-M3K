from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, to_timestamp, lit
from pyspark.sql.types import StructType, StructField, DoubleType, StringType
import os
import sys
import argparse
import logging
import psycopg2
from psycopg2.extras import execute_batch
from datetime import datetime, timedelta
from dotenv import load_dotenv
load_dotenv("/opt/spark-data/.env")

# ============================================================
# Logger
# ============================================================
def setup_logger():
    log_dir = "/tmp/spark-logs"
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    logger = logging.getLogger("spark_batch_job")
    logger.setLevel(logging.INFO)

    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")

    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(fmt)
    ch.flush = sys.stdout.flush

    fh = logging.FileHandler(log_path, mode="w", encoding="utf-8")
    fh.setFormatter(fmt)

    if logger.hasHandlers():
        logger.handlers.clear()
    logger.addHandler(ch)
    logger.addHandler(fh)

    logger.info(f"Logging started: {log_path}")
    return logger


# ============================================================
# PostgreSQL UPSERT (ì‚¬ìš©ì ì •ì˜)
# ============================================================
def upsert_partition(rows, args):
    pg_config = {
        "host": args.pg_host,
        "port": args.pg_port,
        "dbname": args.pg_db,
        "user": args.pg_user,
        "password": args.pg_pass
    }

    conn = psycopg2.connect(**pg_config)
    cur = conn.cursor()

    # í…Œì´ë¸” ìƒì„±
    create_sql = f"""
    CREATE TABLE IF NOT EXISTS {args.pg_table} (
        send_timestamp TIMESTAMPTZ,
        machine TEXT,
        timestamp TEXT,
        usage TEXT,
        PRIMARY KEY (machine, timestamp, usage),
        label INT,
        {','.join([f'col_{i} FLOAT' for i in range(38)])}
    );
    """
    cur.execute(create_sql)

    cols = ["send_timestamp", "machine", "timestamp", "usage", "label"] + [f"col_{i}" for i in range(38)]
    placeholders = ",".join(["%s"] * len(cols))

    upsert_sql = f"""
    INSERT INTO {args.pg_table} ({",".join(cols)})
    VALUES ({placeholders})
    ON CONFLICT (machine, timestamp, usage)
    DO NOTHING;
    """

    batch = []
    BATCH_SIZE = 500

    for row in rows:
        record = [
            row.send_timestamp,
            row.machine,
            row.timestamp,
            row.usage,
            row.label
        ]
        for i in range(38):
            record.append(getattr(row, f"col_{i}"))
        batch.append(tuple(record))

        if len(batch) >= BATCH_SIZE:
            execute_batch(cur, upsert_sql, batch)
            conn.commit()
            batch.clear()

    if batch:
        execute_batch(cur, upsert_sql, batch)
        conn.commit()

    cur.close()
    conn.close()


# ============================================================
# Args
# ============================================================
def parse_args():
    parser = argparse.ArgumentParser(description="Spark batch job for Kafka â†’ PostgreSQL upsert backfill")

    parser.add_argument("--pg-host", default=os.getenv("PG_HOST", "localhost"))
    parser.add_argument("--pg-port", default=os.getenv("PG_PORT", "5432"))
    parser.add_argument("--pg-db", default=os.getenv("PG_DB", "postgres"))
    parser.add_argument("--pg-user", default=os.getenv("PG_USER", "postgres"))
    parser.add_argument("--pg-pass", default=os.getenv("PG_PASS", "postgres"))
    parser.add_argument("--pg-table", default=os.getenv("PG_TABLE", "smd_data_lake"))

    parser.add_argument("--kafka-bootstrap", default=os.getenv("KAFKA_BOOTSTRAP"))
    parser.add_argument("--topic", default=os.getenv("KAFKA_TOPIC"))

    parser.add_argument("--days", type=int, default=1)

    return parser.parse_args()


# ============================================================
# Main
# ============================================================
def main():
    logger = setup_logger()
    args = parse_args()

    day_offset_iso = (datetime.now() - timedelta(days=args.days)).isoformat(timespec="microseconds")
    logger.info(f"Backfill ê¸°ì¤€ì¼: {day_offset_iso}")

    # Kafka JSON schema
    schema = StructType([
        StructField("send_timestamp", StringType(), True),
        StructField("machine", StringType(), True),
        StructField("timestamp", StringType(), True),
        StructField("usage", StringType(), True),
        StructField("label", DoubleType(), True),
        *[StructField(f"col_{i}", DoubleType(), True) for i in range(38)],
    ])

    spark = (
        SparkSession.builder
        .appName("SparkBatchBackfillUpsert")
        .config("spark.sql.session.timeZone", "Asia/Seoul")
        .getOrCreate()
    )

    df = spark.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", args.kafka_bootstrap) \
        .option("subscribe", args.topic) \
        .option("startingOffsets", "earliest") \
        .option("endingOffsets", "latest") \
        .load()

    if df.rdd.isEmpty():
        logger.warning("Kafka í† í”½ ë¹„ì–´ìˆìŒ â€” ì¢…ë£Œ")
        spark.stop()
        return

    json_df = df.selectExpr("CAST(value AS STRING) as json_str") \
                .select(from_json(col("json_str"), schema).alias("data")) \
                .select("data.*")

    # ëŒ€ìƒ ê¸°ê°„ í•„í„°
    filtered_df = json_df.filter(
        to_timestamp(col("send_timestamp")) >= to_timestamp(lit(day_offset_iso))
    )

    # cast timestamp
    final_df = filtered_df.withColumn(
        "send_timestamp",
        to_timestamp(col("send_timestamp"))
    )

    logger.info("ë°ì´í„° ìƒ˜í”Œ")
    final_df.show(10, truncate=False)

    logger.info("PostgreSQL UPSERT ì‹œì‘")
    # âœ… ì €ì¥ ì „ í–‰ ìˆ˜ ì¹´ìš´íŠ¸ ë° ì‹œê°„ ì¸¡ì •
    total_inserted = final_df.count()
    logger.info(f"ğŸ§® ì €ì¥ ì˜ˆì • í–‰ ìˆ˜: {total_inserted}")

    start = datetime.now()

    final_df.foreachPartition(lambda rows: upsert_partition(rows, args))

    elapsed = (datetime.now() - start).total_seconds()
    logger.info(f"UPSERT ì™„ë£Œ â€” ì†Œìš”ì‹œê°„ {elapsed:.1f}s")

    spark.stop()


if __name__ == "__main__":
    main()


# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col, from_json, to_timestamp, lit
# from pyspark.sql.types import StructType, StructField, DoubleType, StringType
# import os
# import sys
# import argparse
# import logging
# from datetime import datetime, timedelta
# from dotenv import load_dotenv
# load_dotenv("/opt/spark-data/.env")

# # -------------------- ë¡œê±° ì„¤ì • -------------------- #
# def setup_logger():
#     # log_dir = "/opt/spark-data/logs"   # âœ… ì ˆëŒ€ê²½ë¡œ ì§€ì •
#     log_dir = "/tmp/spark-logs"   # âœ… í•­ìƒ ì“°ê¸° ê°€ëŠ¥í•œ ë””ë ‰í† ë¦¬
#     os.makedirs(log_dir, exist_ok=True)
#     log_path = os.path.join(log_dir, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

#     logger = logging.getLogger("spark_batch_job")
#     logger.setLevel(logging.INFO)

#     formatter = logging.Formatter(
#         fmt="%(asctime)s [%(levelname)s] %(message)s",
#         datefmt="%Y-%m-%d %H:%M:%S"
#     )

#     # ì½˜ì†” í•¸ë“¤ëŸ¬
#     console_handler = logging.StreamHandler(sys.stdout)
#     console_handler.setFormatter(formatter)
#     console_handler.flush = sys.stdout.flush  # ì¦‰ì‹œ ì¶œë ¥

#     # íŒŒì¼ í•¸ë“¤ëŸ¬
#     file_handler = logging.FileHandler(log_path, mode="w", encoding="utf-8")
#     file_handler.setFormatter(formatter)

#     if logger.hasHandlers():
#         logger.handlers.clear()

#     logger.addHandler(console_handler)
#     logger.addHandler(file_handler)

#     logger.info(f"ğŸ§¾ Logging started: {log_path}")
#     return logger


# # -------------------- ì¸ì íŒŒì„œ -------------------- #

# def parse_args():
#     parser = argparse.ArgumentParser(description="Spark batch job for Kafka â†’ PostgreSQL backfill")

#     parser.add_argument("--pg-host", default=os.getenv("PG_HOST", "localhost"), help="PostgreSQL host")
#     parser.add_argument("--pg-port", default=os.getenv("PG_PORT", "5432"), help="PostgreSQL port")
#     parser.add_argument("--pg-db", default=os.getenv("PG_DB", "postgres"), help="PostgreSQL database name")
#     parser.add_argument("--pg-user", default=os.getenv("PG_USER", "postgres"), help="PostgreSQL username")
#     parser.add_argument("--pg-pass", default=os.getenv("PG_PASS", "postgres"), help="PostgreSQL password")
#     parser.add_argument("--pg-table", default=os.getenv("PG_TABLE", "smd_data_lake"), help="Target PostgreSQL table name")

#     parser.add_argument("--kafka-bootstrap", default=os.getenv("KAFKA_BOOTSTRAP", "kafka.kafka.svc.cluster.local:9092"), help="Kafka bootstrap servers")
#     parser.add_argument("--topic", default=os.getenv("KAFKA_TOPIC", "server-machine-usage"), help="Kafka topic to read from")
#     parser.add_argument("--days", type=int, default=int(os.getenv("BACKFILL_DAYS", "1")), help="Number of days to backfill (default: 1)")

#     return parser.parse_args()


# # -------------------- ë©”ì¸ ì‹¤í–‰ -------------------- #
# def main():
#     logger = setup_logger()
#     args = parse_args()

#     day_offset_iso = (datetime.now() - timedelta(days=args.days)).isoformat(timespec="microseconds")

#     logger.info(f"ğŸ“… Backfill ê¸°ì¤€ì¼: {day_offset_iso}")
#     logger.info(f"ğŸ¯ Kafka topic: {args.topic}")
#     logger.info(f"ğŸ’¾ PostgreSQL: {args.pg_host}:{args.pg_port}/{args.pg_db} â†’ {args.pg_table}")

#     # Kafka JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
#     schema = StructType([
#         StructField("send_timestamp", StringType(), True),
#         StructField("machine", StringType(), True),
#         StructField("timestamp", StringType(), True),
#         StructField("usage", StringType(), True),
#         StructField("label", DoubleType(), True),
#         *[StructField(f"col_{i}", DoubleType(), True) for i in range(38)],
#     ])

#     spark = (
#         SparkSession.builder
#         .appName("SparkBatchBackfill")
#         .config("spark.sql.session.timeZone", "Asia/Seoul")  # âœ… í•œêµ­ ì‹œê°„ ëª…ì‹œ
#         .getOrCreate()
#     )

#     logger.info("ğŸš€ Kafka ë°ì´í„° ì½ê¸° ì‹œì‘")
#     df = spark.read \
#         .format("kafka") \
#         .option("kafka.bootstrap.servers", args.kafka_bootstrap) \
#         .option("subscribe", args.topic) \
#         .option("startingOffsets", "earliest") \
#         .option("endingOffsets", "latest") \
#         .load()

#     if df.rdd.isEmpty():
#         logger.warning("âš ï¸ Kafka í† í”½ì—ì„œ ì½ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
#         spark.stop()
#         sys.exit(0)

#     json_df = df.selectExpr("CAST(value AS STRING) as json_str") \
#         .select(from_json(col("json_str"), schema).alias("data")) \
#         .select("data.*")

#     filtered_df = json_df.filter(
#         to_timestamp(col("send_timestamp")) >= to_timestamp(lit(day_offset_iso))
#     )

#     # Sparkì—ì„œ ì¤‘ë³µ ì œê±° í›„ ì €ì¥ (upsert ë¥¼ spark ì—ì„œ ë¯¸ë¦¬ í•˜ëŠ” íš¨ê³¼ì„. í•˜ì§€ë§Œ, í† í”½ì—ì„œ ì½ì–´ì˜¨ ë°ì´í„° ë‚´ì—ì„œë§Œ íš¨ê³¼ ì ìš©ë¨)
#     dedup_df = filtered_df.dropDuplicates(["machine", "timestamp", "usage"])

#     # ğŸ”§ PostgreSQL ì €ì¥ ì „ì— íƒ€ì… ìºìŠ¤íŒ… ì¶”ê°€
#     dedup_df = dedup_df.withColumn("send_timestamp", to_timestamp(col("send_timestamp")))

#     logger.info("ğŸ“Š ë°ì´í„° ì˜ˆì‹œ (ìµœëŒ€ 10ê°œ)")
#     dedup_df.show(10, truncate=False)

#     jdbc_url = f"jdbc:postgresql://{args.pg_host}:{args.pg_port}/{args.pg_db}"

#     # âœ… ì €ì¥ ì „ í–‰ ìˆ˜ ì¹´ìš´íŠ¸ ë° ì‹œê°„ ì¸¡ì •
#     total_inserted = dedup_df.count()
#     logger.info(f"ğŸ§® ì €ì¥ ì˜ˆì • í–‰ ìˆ˜: {total_inserted}")

#     start_time = datetime.now()

#     try:
#         logger.info("ğŸš€ PostgreSQL ì €ì¥ ì‹œì‘")

#         # -------------------------------
#         # Spark â†’ PostgreSQL ì €ì¥ ì‹¤í–‰
#         # -------------------------------
#         dedup_df.write \
#             .format("jdbc") \
#             .option("url", jdbc_url) \
#             .option("dbtable", args.pg_table) \
#             .option("user", args.pg_user) \
#             .option("password", args.pg_pass) \
#             .option("driver", "org.postgresql.Driver") \
#             .mode("append") \
#             .save()

#         # -------------------------------
#         # ì €ì¥ ì™„ë£Œ í›„ ë¡œê¹…
#         # -------------------------------
#         elapsed = (datetime.now() - start_time).total_seconds()
#         logger.info(f"âœ… PostgreSQL ì €ì¥ ì™„ë£Œ: {total_inserted} rows / {elapsed:.2f}ì´ˆ / í‰ê·  {total_inserted/elapsed:.1f} row/sec")
#         logger.info(f"âœ… PostgreSQL ì €ì¥ í…Œì´ë¸”ëª…: {args.pg_table}")

#     except Exception as e:
#         msg = str(e)
#         # -------------------------------
#         # ì£¼ìš” ì˜ˆì™¸ ë¶„ê¸°
#         # -------------------------------
#         if "does not exist" in msg or "UndefinedTable" in msg:
#             logger.error(f"âŒ PostgreSQL í…Œì´ë¸” '{args.pg_table}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
#         elif "Connection refused" in msg or "Communications link failure" in msg:
#             logger.error("âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨ â€” DB ì ‘ì† ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
#         elif "password authentication failed" in msg:
#             logger.error("âŒ PostgreSQL ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ ì‹¤íŒ¨")
#         else:
#             logger.exception(f"ğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ ë°œìƒ: {msg}")
#             raise

#     finally:
#         # -------------------------------
#         # Spark ì„¸ì…˜ ì¢…ë£Œ (í•­ìƒ ì‹¤í–‰)
#         # -------------------------------
#         try:
#             spark.stop()
#             logger.info("ğŸ Spark ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ")
#         except Exception as e:
#             logger.warning(f"âš ï¸ Spark ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")


# if __name__ == "__main__":
#     main()
