from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, to_timestamp, lit
from pyspark.sql.types import StructType, StructField, DoubleType, StringType
import os
import sys
import argparse
import logging
from datetime import datetime, timedelta
from dotenv import load_dotenv
load_dotenv("/opt/spark-data/.env")

# -------------------- ë¡œê±° ì„¤ì • -------------------- #
def setup_logger():
    # log_dir = "/opt/spark-data/logs"   # âœ… ì ˆëŒ€ê²½ë¡œ ì§€ì •
    log_dir = "/tmp/spark-logs"   # âœ… í•­ìƒ ì“°ê¸° ê°€ëŠ¥í•œ ë””ë ‰í† ë¦¬
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    logger = logging.getLogger("spark_batch_job")
    logger.setLevel(logging.INFO)

    formatter = logging.Formatter(
        fmt="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.flush = sys.stdout.flush  # ì¦‰ì‹œ ì¶œë ¥

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_path, mode="w", encoding="utf-8")
    file_handler.setFormatter(formatter)

    if logger.hasHandlers():
        logger.handlers.clear()

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    logger.info(f"ğŸ§¾ Logging started: {log_path}")
    return logger


# -------------------- ì¸ì íŒŒì„œ -------------------- #

def parse_args():
    parser = argparse.ArgumentParser(description="Spark batch job for Kafka â†’ PostgreSQL backfill")

    parser.add_argument("--pg-host", default=os.getenv("PG_HOST", "localhost"), help="PostgreSQL host")
    parser.add_argument("--pg-port", default=os.getenv("PG_PORT", "5432"), help="PostgreSQL port")
    parser.add_argument("--pg-db", default=os.getenv("PG_DB", "postgres"), help="PostgreSQL database name")
    parser.add_argument("--pg-user", default=os.getenv("PG_USER", "postgres"), help="PostgreSQL username")
    parser.add_argument("--pg-pass", default=os.getenv("PG_PASS", "postgres"), help="PostgreSQL password")
    parser.add_argument("--pg-table", default=os.getenv("PG_TABLE", "smd_data_lake"), help="Target PostgreSQL table name")

    parser.add_argument("--kafka-bootstrap", default=os.getenv("KAFKA_BOOTSTRAP", "kafka.kafka.svc.cluster.local:9092"), help="Kafka bootstrap servers")
    parser.add_argument("--topic", default=os.getenv("KAFKA_TOPIC", "server-machine-usage"), help="Kafka topic to read from")
    parser.add_argument("--days", type=int, default=int(os.getenv("BACKFILL_DAYS", "1")), help="Number of days to backfill (default: 1)")

    return parser.parse_args()


# -------------------- ë©”ì¸ ì‹¤í–‰ -------------------- #
def main():
    logger = setup_logger()
    args = parse_args()

    yesterday_iso = (datetime.now() - timedelta(days=args.days)).isoformat(timespec="microseconds")

    logger.info(f"ğŸ“… Backfill ê¸°ì¤€ì¼: {yesterday_iso}")
    logger.info(f"ğŸ¯ Kafka topic: {args.topic}")
    logger.info(f"ğŸ’¾ PostgreSQL: {args.pg_host}:{args.pg_port}/{args.pg_db} â†’ {args.pg_table}")

    # Kafka JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
    schema = StructType([
        StructField("send_timestamp", StringType(), True),
        StructField("machine", StringType(), True),
        StructField("timestamp", DoubleType(), True),
        StructField("usage", StringType(), True),
        StructField("label", DoubleType(), True),
        *[StructField(f"col_{i}", DoubleType(), True) for i in range(38)],
    ])

    spark = (
        SparkSession.builder
        .appName("SparkBatchBackfill")
        .config("spark.sql.session.timeZone", "Asia/Seoul")  # âœ… í•œêµ­ ì‹œê°„ ëª…ì‹œ
        .getOrCreate()
    )

    logger.info("ğŸš€ Kafka ë°ì´í„° ì½ê¸° ì‹œì‘")
    df = spark.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", args.kafka_bootstrap) \
        .option("subscribe", args.topic) \
        .option("startingOffsets", "earliest") \
        .option("endingOffsets", "latest") \
        .load()

    if df.rdd.isEmpty():
        logger.warning("âš ï¸ Kafka í† í”½ì—ì„œ ì½ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        spark.stop()
        sys.exit(0)

    json_df = df.selectExpr("CAST(value AS STRING) as json_str") \
        .select(from_json(col("json_str"), schema).alias("data")) \
        .select("data.*")

    filtered_df = json_df.filter(
        to_timestamp(col("send_timestamp")) >= to_timestamp(lit(yesterday_iso))
    )

    # ğŸ”§ PostgreSQL ì €ì¥ ì „ì— íƒ€ì… ìºìŠ¤íŒ… ì¶”ê°€
    filtered_df = filtered_df.withColumn("send_timestamp", to_timestamp(col("send_timestamp")))

    logger.info("ğŸ“Š ë°ì´í„° ì˜ˆì‹œ (ìµœëŒ€ 10ê°œ)")
    filtered_df.show(10, truncate=False)

    jdbc_url = f"jdbc:postgresql://{args.pg_host}:{args.pg_port}/{args.pg_db}"

    # âœ… ì €ì¥ ì „ í–‰ ìˆ˜ ì¹´ìš´íŠ¸ ë° ì‹œê°„ ì¸¡ì •
    total_inserted = filtered_df.count()
    logger.info(f"ğŸ§® ì €ì¥ ì˜ˆì • í–‰ ìˆ˜: {total_inserted}")

    start_time = datetime.now()

    try:
        logger.info("ğŸš€ PostgreSQL ì €ì¥ ì‹œì‘")

        # -------------------------------
        # Spark â†’ PostgreSQL ì €ì¥ ì‹¤í–‰
        # -------------------------------
        filtered_df.write \
            .format("jdbc") \
            .option("url", jdbc_url) \
            .option("dbtable", args.pg_table) \
            .option("user", args.pg_user) \
            .option("password", args.pg_pass) \
            .option("driver", "org.postgresql.Driver") \
            .mode("append") \
            .save()

        # -------------------------------
        # ì €ì¥ ì™„ë£Œ í›„ ë¡œê¹…
        # -------------------------------
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info(f"âœ… PostgreSQL ì €ì¥ ì™„ë£Œ: {total_inserted} rows / {elapsed:.2f}ì´ˆ / í‰ê·  {total_inserted/elapsed:.1f} row/sec")
        logger.info(f"âœ… PostgreSQL ì €ì¥ í…Œì´ë¸”ëª…: {args.pg_table}")

    except Exception as e:
        msg = str(e)
        # -------------------------------
        # ì£¼ìš” ì˜ˆì™¸ ë¶„ê¸°
        # -------------------------------
        if "does not exist" in msg or "UndefinedTable" in msg:
            logger.error(f"âŒ PostgreSQL í…Œì´ë¸” '{args.pg_table}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
        elif "Connection refused" in msg or "Communications link failure" in msg:
            logger.error("âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨ â€” DB ì ‘ì† ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        elif "password authentication failed" in msg:
            logger.error("âŒ PostgreSQL ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ ì‹¤íŒ¨")
        else:
            logger.exception(f"ğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ ë°œìƒ: {msg}")
            raise

    finally:
        # -------------------------------
        # Spark ì„¸ì…˜ ì¢…ë£Œ (í•­ìƒ ì‹¤í–‰)
        # -------------------------------
        try:
            spark.stop()
            logger.info("ğŸ Spark ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ Spark ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
